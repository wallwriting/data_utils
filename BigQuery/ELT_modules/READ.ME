NOTE: procs can be copy/pasted. However, the dataset name in the 1st line of the CREATE OR REPLACE PROCEDURE statement needs to be changed from 'test' to your dataset name

This folder contains ELT modules to run in BigQuery. Each module takes data from a specified table, performas its task, and outputs the results into another table. Users only need to specify the input project/dataset/table and output project/dataset/table as well as any join fields. No information about the schema is necessary. Each module is entirely self-contained (no additional libraries are needed, and no outside procs/functions are called), can be run in any combination, and in any order.

Modules:

deSTRUCT
  - Flattens all STRUCT datatypes and creates a separate table for each
  - Creates a root table with the remaining non-STRUCT fields
  - User selects a key field to be able to JOIN all tables together
  Arguments:
      varSrcPrj STRING: The name of the source project--must be put in quotes (eg 'MY-SOURCE-PROJECT')
      varSrcDs STRING: The source dataset--must be put in quotes (eg 'MY-SOURCE-DATASET')
      varSrcTbl STRING: The target dataset--must be put in quotes (eg 'MY-TARGET-DATASET')
      varTgtDs STRING: The target dataset--must be put in quotes (eg 'MY-TARGET-DATASET')
      varTblJoinKey STRING: The join column that will be included in all flattened tables created by the proc--must be put in quotes (eg 'my_join_column')

disARRAY
  - Flattens all ARRAY datatypes and creates a separate table for each
  - Creates a root table with the remaining non-STRUCT fields
  - User selects a key field to be able to JOIN all tables together
  Arguments:
      varSrcPrj STRING: The name of the source project--must be put in quotes (eg 'MY-SOURCE-PROJECT')
      varSrcDs STRING: The source dataset--must be put in quotes (eg 'MY-SOURCE-DATASET')
      varSrcTbl STRING: The target dataset--must be put in quotes (eg 'MY-TARGET-DATASET')
      varTgtDs STRING: The target dataset--must be put in quotes (eg 'MY-TARGET-DATASET')
      varTblJoinKey STRING: The join column that will be included in all flattened tables created by the proc--must be put in quotes (eg 'my_join_column')

NULLdeCOLUMNizer
  - Drops all NULL fields from a table
  - ARRAY and STRUCT fields are always created in the output table as they can’t be checked for NULLs without flattening them
    Arguments:
      varSrcPrj STRING: The name of the source project--must be put in quotes (eg 'MY-SOURCE-PROJECT')
      varSrcDs STRING: The source dataset--must be put in quotes (eg 'MY-SOURCE-DATASET')
      varSrcTbl STRING: The target dataset--must be put in quotes (eg 'MY-TARGET-DATASET')
      varTgtDs STRING: The target dataset--must be put in quotes (eg 'MY-TARGET-DATASET')

JOINer
  - Joins together two or more tables
  - One “base” table is selected--for joining more than two tables, each table is joined back to the base table (ie Table A joins to Table B, Table A joins to Table C, Table A joins to Table D, etc.)
  - Can select INNER, LEFT, RIGHT, FULL OUTER, and CROSS joins
    Arguments:
      varSourceProject STRING: The name of the source project--must be put in quotes (eg 'MY-SOURCE-PROJECT')
      varSourceDataset STRING: The source dataset--must be put in quotes (eg 'MY-SOURCE-DATASET')
      varTargetDataset STRING: The target dataset--must be put in quotes (eg 'MY-TARGET-DATASET') 
      varTargetTable STRING: The target table--must be put in quotes(eg 'MY-TARGET-TABLE')
      varMainTable STRING: The parent table that all other tables will join to--must be put in quotes (eg 'MY_MAIN_TABLE_NAME'
      varJoinKey STRING: The join column of the parent table--must be put in quotes (eg 'my_join_column')
      varJoinType STRING: The type of join--must be put in quotes--only one one type of join is allowed per call (eg 'INNER', 'LEFT', 'RIGHT, 'FULL OUTER', or 'CROSS')
      varRightTableKeyArray ARRAY<STRUCT<right_table_name STRING, right_key_field STRING>>: A key/value pair list of each child table and the join key (eg [(TableB', 'KeyB'), ('TableC', 'KeyC'), ('TableD', 'KeyD')])
  - An additional version of this procedure, JOINer_Operation, includes an operation flag in the right side tables to filter out any child tables in situations where the value in the parent table is still valid but the value in the child as been deleted
  - Another version of this procedure, JOINer_max, includes a subquery for each right side table that gets the max version of each key value. This is to account for scenarios where multiple transactions occur on the same source record in between batch runs. This also comes in a version that includes the operation flag.

UPSERTer
  - Takes two tables and performs an UPSERT for all fields whose names match from source to target
  - Can handle more complex scenarios than MERGE, including cases where the same record has multiple updates and handling deletes even in cases where the source table is only a subset of the target table
  - Updates are converted to deletes and inserts to handle cases where many fields are updated
  - Proc will only consider column names that match in the source and target tables 
  - Proc doesn't need an explicit value for deletes due to the way the IF statement is structured
    Arguments:
      source_project STRING: The name of the source project--must be put in quotes (eg 'MY-SOURCE-PROJECT')
      source_dataset STRING:  The source dataset--must be put in quotes (eg 'MY-SOURCE-DATASET')
      source_table STRING: The source table--must be put in quuotes (eg 'MY_SOURCE_TABLE_NAME')
      source_version STRING: The column used to version a record--this is often a timestamp or sequence number--column name must match between source and target and must be put in quotes (eg 'my_version_column')
      source_dml_indicator STRING: The column used to indicate the type of dml operation to be undertaken--must be put in quotes (eg 'my_dml_indicator_column')
      join_key STRING: The join column of both the source and target table table--must be the same name on both source and target and must be put in quotes (eg 'my_join_column')
      target_dataset STRING: The target dataset--must be put in quotes (eg 'MY-TARGET-DATASET')
      target_table STRING: The target table--must be put in quuotes (eg 'MY_TARGET_TABLE_NAME')
      update_var STRING: The value in the source_dml_indicator column that represents an update (eg 'U' or 'UPDATE)
      insert_var STRING: The value in the source_dml_indicator column that represents an insert (eg 'I' or 'INSERT')

COLUMNpicker
  - Create a new table using a specified subset of columns from a source table
  - New column names in the target table must also be specified
  Arguments:
    varSourceProject STRING
    varSourceDataset STRING
    varSourceTable STRING
    varTargetDataset STRING
    varTargetTable STRING
    varSelectColumnArray ARRAY<STRUCT<source_column_name STRING, target_column_name STRING>>): Must be sent as a key/value pair with the source column name and a target column name (eg [('source_col1', 'target_col1'), ('source_col2', 'target_col2'), ('source_col3', 'target_col3')]

KEYmaster
  - Creates a target table along with a new key field based on a specified list of fields
  - User passes a set of fields--the values will be converted to string and concatenated together, then a farm fingerprint hash will be applied
  - The same concatenated string value will always produce the same hash--this means it can be used prior to running a JOINer in cases where tables have multi-column join keys
  Arguments:
      varSourceProject STRING
      varSourceDataset STRING
      varSourceTable STRING
      varTargetDataset STRING
      varTargetTable STRING
      varTargetKey STRING
      varHashColumnArray ARRAY<STRING>: The list of columns you want to use for the hash key (eg ['col1', 'col2', 'col3']) 

FLATTENator
  - Unnests ARRAY and STRUCT datatypes
  - This does the work of both the deSTRUCT and disARRAY procs in one pass. In the future, this proc will receive all updates.
  Arguments:
      varSrcPrj STRING: The name of the source project--must be put in quotes (eg 'MY-SOURCE-PROJECT')
      varSrcDs STRING: The source dataset--must be put in quotes (eg 'MY-SOURCE-DATASET')
      varSrcTbl STRING: The source table--must be put in quotes (eg 'MY-SOURCE-table')
      varTgtDs STRING: The target dataset--must be put in quotes (eg 'MY-TARGET-DATASET')
      varTblJoinKey STRING: The join column that will be included in all flattened tables created by the proc--must be put in quotes (eg 'my_join_column')

BatchCDC
  - Takes records from multiple source tables that have been inserted after the last batch run of an ELT process
  - This is used as the first step of an ELT pipeline to select only change data records. It requires a metadata table that stores the timestamp for the most recent batch run
  Arguments:
    argSourceTableArray ARRAY<STRUCT<col_source_project_name STRING, col_source_dataset STRING, col_source_table_name STRING, col_source_timestamp_field_name STRING, col_target_table_name STRING>>: An array of struct--each struct has information about 1 source and 1 target table, can pass multiple values in the array to do this for multiple tables. The timestamp field in this section does not have to be an actual timestamp, but it must be sometype of calendar field (date, datetime, etc.) that can be cast to a timestamp. Use brackets then parentheses
    argMetaTableStruct STRUCT<meta_project STRING, meta_dataset STRING, meta_table_name STRING, meta_key STRING, meta_timestamp STRING>: The metadata table with the batch run information--the table must have a key field that is an integer and a timestamp field--use parentheses only
    argTargetDataset STRING: The dataset of the target table--this means this can only be done for a single target dataset, this canbe modified if each target table needs its own dataset


NullDELETEr
  - Deletes any records that have NULL values in any of the columns passed in the parameter
  - This is one of the last steps of an ELT pipeline. In situations were a single target record is created from multiple source tables and/or systems, this removes any records from a table where there are NULL values in the columns stated. Such records are assumed to have incomplete source data and is therefore not valid. As data from each source lands, the record will continue to get processed in the pipeline and removed by this procecure. Once all of the source tables have landed data, then the record is considered valid and wouldn't be deleted by this process
  Arguments:
    varDataset STRING: The dataset where the table resides
    varTable STRING: The name of the table
    varNullColumnArray ARRAY<STRING>: An array of all of the columns that are to be checked for NULL values


INTERNALizer
  - This creates an external table based on a directory structure specified by the dev and loads the data into an internal table.
  - This proc is solely to work around the issues of how datastream creates its directory structure when replicates into GCS. The directory doesn't allow for partitioned external tables, which causes performance issues at scale. This allows devs to create an external table just on a subdirectory or file to keep up performance and load the data into a copy of the full source data (ie all history) into  an internal table. This allows for the versitility of using GCS as the data lake (otherwise you could replicate directly into BigQuery), while maintaining overall performance.
  - In cases where the same record has multiple database transactions in a batch, this script will pick the most recent one.
    Arguments:
      argSourceProject STRING: source project name
      argSourceRegion STRING: source region, needed for the biglake table creation
      argConnection STRING: source connection, needed for the biglake table. Needs to be created beforehand
      argFormat STRING: format of the files (json, avro, csv, ect.)
      argSourceUriBase ARRAY<STRING>: An array of the source uris to be used. Needs a filename or wildcard at the end
      argTargetDataset STRING: dataset where the external and internal target tables will exist
      argTargetExternalTable STRING: name for the external table you want to create
      argTargetInternalTable STRING: name for the internal table you want to load data into
      argPrimaryKeyArray ARRAY<STRING>: the logical pk of the external table, used for upserts

INTERNALizer_incremental
  - Same as above, except this version automatically picks up all valid uri subdirectories for a table going back a period of time you specify
  - This is used in cases where passing hard-coded arguments for the uri wouldn't be feasible.
  - Note there are two extra arguments here
    Arguments:
      argSourceProject STRING: source project name
      argSourceRegion STRING: source region, needed for the biglake table creation
      argConnection STRING: source connection, needed for the biglake table. Needs to be created beforehand
      argFormat STRING: format of the files (json, avro, csv, ect.)
      argSourceUriBase ARRAY<STRING>: An array of the source uris to be used. Needs a filename or wildcard at the end
      argTargetDataset STRING: dataset where the external and internal target tables will exist
      argTargetExternalTable STRING: name for the external table you want to create
      argTargetInternalTable STRING: name for the internal table you want to load data into
      argPrimaryKeyArray ARRAY<STRING>: the logical pk of the external table, used for upserts
      argDatePart STRING, --the part of the date you want to increment by. This can only be DAY, HOUR, or MINUTE
      argGoBackInterval INT64 --how far back you want the datapart to go back. For example if date part is DAY, and this value is 3, that means you want it to go back three days from today.


unJSONer
  - flattens any json columns in a table, placing each into their own tables with schema
  - creates tables if they don't exist, inserts into tables if they do--this might need to be changed so that it always drops and recreates the table
  Arguments:
    argSourceProject STRING
    argSourceDataset STRING
    argSourceTable STRING
    argSourceRecordId STRING: the primary key identifier for rows in the source table
    argTargetProject STRING
    argTargetDataset STRING
